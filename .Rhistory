payouts_2017 <- all_payouts_decade %>%
filter(fiscal_year == "2017")
# Write the code to create a dataframe of payouts from 2006-2016
payouts_2006_2016 <- all_payouts_decade %>%
filter(fiscal_year < "2017")
# Sum the total number of payouts from 2006-2016 (satisfies "five times as many as a decade before")
payouts_2006_2016 %>%
summarise(
count=n())
# Determine the percent change in payouts through the whole 10-year period (satisfies "while the total number of payouts has dipped over time")
yearly_payout_count <- all_payouts_decade %>%
group_by(fiscal_year) %>%
summarise(count=n()) %>%
arrange(fiscal_year)
yearly_payout_count %>%
summarise(
total_payouts_10_years = sum(count)
)
yearly_payout_count %>%
summarise(
percent_change = ((1369-1468)/18703)*100
)
# Find the average amount of payouts from the decade 2006-2016 (satisfies "the average amount has increased")
payouts_2006_2016 %>%
summarise(
total_mean_pay_amount = mean(amount)
)
payouts_2006_2016 %>%
filter(fiscal_year == "2006") %>%
summarise(
mean_pay_amount_2006 = mean(amount)
)
payouts_2006_2016 %>%
filter(fiscal_year == "2016") %>%
summarise(
mean_pay_amount_2016 = mean(amount)
)
yearly_payout_count %>%
summarise(
percent_change = ((94230.15-25345.32)/37567.26)*100)
# Put code to reverse engineer sentence here
# Filter to find the total number of payouts over $1 million in 2017. (satisfies "Last budget year, the city paid out 30 settlements of $1 million or more")
payouts_2017 %>%
filter(amount >= 1000000)
# Sum the total number of payouts greater the $1 million from 2006-2016 (satisfies "five times as many as a decade before")
payouts_2006_2016 %>%
filter(fiscal_year == "2007" & amount >= 1000000) %>%
select(fiscal_year, amount, case_name)
# Put code to reverse engineer sentence here
police_department_payouts_2016 <- all_payouts_decade %>%
filter(department == "Police Department" & fiscal_year == 2016 & amount > 7000000)
police_department_payouts_2016 %>%
filter(claim_case_number != "BC453870") %>%
summarize(total = sum(amount))
# Display results of code below this codeblock
# "Thousands of legal battles" (using payouts as a proxy)
all_payouts_decade %>%
summarise(
count=n())
# Total amount of payouts over 10 years = 880687820
# Total Police Department Sum over 10 years
all_payouts_decade %>%
filter(department == "Police Department") %>%
group_by(department) %>%
summarize(total = sum(amount))
# Percentage of payouts that have been attributed to the police department across the last decade
(369018871/880687820)*100
# finding the url
all_states <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
all_states <- state_url %>%
read_html() %>%
html_table()
# finding the url
state_url <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
all_states <- state_url %>%
read_html() %>%
html_table()
# table from the url list
state_urls <- state_url[[1]]
# empty dataframe for state info
ppp_loans_per_capita <- tibble()
# for loop
for (row_number in 1:nrow(state_urls)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- state_urls %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$sector_url
# dataframe with each individual state
individual_states <- url %>%
read_html() %>%
html_table()
individual_state_info <- individual_states[[1]]
# bind rows
state_ppp_all <- state_ppp_all %>%
bind_rows(individual_state_info)
employment_info <- employment_info[[1]] %>%
clean_names() %>%
slice(2) %>%
bind_cols(each_row_df) %>%
mutate(nov_2021 = parse_number(nov_2021)) %>%
rename(nov_2021_employees = nov_2021) %>%
select(sector,description,nov_2021_employees)
}
# finding the url
state_url <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
all_states <- state_url %>%
read_html() %>%
html_table()
# table from the url list
state_urls <- state_url[[1]]
# empty dataframe for state info
ppp_loans_per_capita <- tibble()
# for loop
for (row_number in 1:nrow(state_urls)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- state_urls %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$sector_url
# dataframe with each individual state
individual_states <- url %>%
read_html() %>%
html_table()
individual_state_info <- individual_states[[1]]
# bind rows
state_ppp_all <- state_ppp_all %>%
bind_rows(individual_state_info)
}
state_url <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
all_states <- state_url %>%
read_html() %>%
html_table()
# table from the url list
state_urls <- state_url[[1]]
# empty dataframe for state info
ppp_loans_per_capita <- tibble()
# for loop
for (row_number in 1:nrow(state_urls)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- state_urls %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$sector_url
# dataframe with each individual state
individual_states <- url %>%
read_html() %>%
html_table() }
# finding the url
state_url <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
all_states <- state_url %>%
read_html() %>%
html_table()
# table from the url list
state_urls <- state_url[[1]]
# empty dataframe for state info
ppp_loans_per_capita <- tibble()
# for loop
for (row_number in 1:nrow(state_urls)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- state_urls %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$sector_url
# dataframe with each individual state
individual_states <- url %>%
read_html() %>%
html_table()
individual_state_info <- individual_states[[1]]
# bind rows
ppp_loans_per_capita <- ppp_loans_per_capita %>%
bind_rows(individual_state_info)
}
# finding the url
state_url <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
state_urls <- state_url %>%
read_html() %>%
html_table()
# table from the url list
state_urls <- state_url[[1]]
# empty dataframe for state info
ppp_loans_per_capita <- tibble()
# for loop
for (row_number in 1:nrow(state_urls)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- state_urls %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$sector_url
# dataframe with each individual state
individual_states <- url %>%
read_html() %>%
html_table()
individual_state_info <- individual_states[[1]]
# bind rows
ppp_loans_per_capita <- ppp_loans_per_capita %>%
bind_rows(individual_state_info)
}
# finding the url
state_url <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
state_urls <- state_url %>%
read_html() %>%
html_table()
# table from the url list
state_urls <- state_url[[1]]
# empty dataframe for state info
all_state_ppp <- tibble()
# for loop
for (row_number in 1:nrow(state_urls)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- state_urls %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$url
# dataframe with each individual state
individual_states_info <- url %>%
read_html() %>%
html_table()
individual_state_info <- individual_states_info[[1]]
# bind rows
all_state_ppp <- all_state_ppp %>%
bind_rows(individual_state_info)
}
# finding the url
state_url <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
state_urls <- state_url %>%
read_html() %>%
html_table()
# table from the url list
state_urls <- state_urls[[1]]
# empty dataframe for state info
all_state_ppp <- tibble()
# for loop
for (row_number in 1:nrow(state_urls)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- state_urls %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$url
# dataframe with each individual state
individual_states_info <- url %>%
read_html() %>%
html_table()
individual_state_info <- individual_states_info[[1]]
# bind rows
all_state_ppp <- all_state_ppp %>%
bind_rows(individual_state_info)
}
ppp_loans_per_capita <- ppp_loans_per_capita %>%
mutate(loans_per_capita = total_ppp_loans/population*100000)
# finding the url
state_url <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
state_urls <- state_url %>%
read_html() %>%
html_table()
# table from the url list
state_urls <- state_urls[[1]]
# empty dataframe for state info
all_state_ppp <- tibble()
# for loop
for (row_number in 1:nrow(state_urls)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- state_urls %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$url
# dataframe with each individual state
individual_states_info <- url %>%
read_html() %>%
html_table()
individual_state_info <- individual_states_info[[1]]
# bind rows
all_state_ppp <- all_state_ppp %>%
bind_rows(individual_state_info)
all_state_ppp <- all_state_ppp %>%
mutate(loans_per_capita = total_ppp_loans/population*100000)
# finding the url
state_url <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
state_urls <- state_url %>%
read_html() %>%
html_table()
# table from the url list
state_urls <- state_urls[[1]]
# empty dataframe for state info
all_state_ppp <- tibble()
# for loop
for (row_number in 1:nrow(state_urls)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- state_urls %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$url
# dataframe with each individual state
individual_states_info <- url %>%
read_html() %>%
html_table()
individual_state_info <- individual_states_info[[1]]
# bind rows
all_state_ppp <- all_state_ppp %>%
bind_rows(individual_state_info)
}
all_state_ppp <- all_state_ppp %>%
mutate(loans_per_capita = total_ppp_loans/population*100000)
all_state_ppp
```{r}
state_url <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
state_urls <- state_url %>%
read_html() %>%
html_table()
state_url <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
state_urls <- state_url %>%
read_html() %>%
html_table()
# table from the url list
state_urls <- state_urls[[1]]
# empty dataframe for state info
all_state_ppp <- tibble()
state_url <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
state_urls <- state_url %>%
read_html() %>%
html_table()
# table from the url list
state_urls <- state_urls[[1]]
# empty dataframe for state info
all_state_ppp <- tibble()
# for loop
for (row_number in 1:nrow(state_urls)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- state_urls %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$url
# dataframe with each individual state
individual_states_info <- url %>%
read_html() %>%
html_table() }
state_url <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
state_urls <- state_url %>%
read_html() %>%
html_table()
# table from the url list
state_urls <- state_urls[[1]]
# empty dataframe for state info
all_state_ppp <- tibble()
# for loop
for (row_number in 1:nrow(state_urls)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- state_urls %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$url
# dataframe with each individual state
individual_states_info <- url %>%
read_html() %>%
html_table()
individual_state_info <- individual_states_info[[1]]
# bind rows
all_state_ppp <- all_state_ppp %>%
bind_rows(individual_state_info)
}
# finding the url
state_url <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
state_urls <- state_url %>%
read_html() %>%
html_table()
# table from the url list
state_urls <- state_urls[[1]]
# empty dataframe for state info
all_state_ppp <- tibble()
# for loop
for (row_number in 1:nrow(state_urls)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- state_urls %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$url
# dataframe with each individual state
individual_states_info <- url %>%
read_html() %>%
html_table()
individual_state_info <- individual_states_info[[1]]
# bind rows
all_state_ppp <- all_state_ppp %>%
bind_rows(individual_state_info)
}
all_state_ppp <- all_state_ppp %>%
mutate(loans_per_capita = total_ppp_loans/population*100000) %>%
arrange(desc(loans_per_capita))
all_state_ppp
# finding the url
all_states_url <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
all_state_urls <- all_states_url %>%
read_html() %>%
html_table()
# table from the url list
state_urls <- all_state_urls[[1]]
# empty dataframe for state info
all_state_ppp <- tibble()
# for loop
for (row_number in 1:nrow(all_state_urls)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- state_urls %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$url
# dataframe with each individual state
info_each_state <- url %>%
read_html() %>%
html_table()
info_each_state <- info_each_state[[1]]
# bind rows
all_state_ppp <- all_state_ppp %>%
bind_rows(info_each_state)
}
# finding the url
all_states_url <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
all_state_urls <- all_states_url %>%
read_html() %>%
html_table()
# table from the url list
state_urls <- all_state_urls[[1]]
# empty dataframe for state info
all_state_ppp <- tibble()
# for loop
for (row_number in 1:nrow(all_state_urls)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- all_state_urls %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$url
# dataframe with each individual state
info_each_state <- url %>%
read_html() %>%
html_table()
info_each_state <- info_each_state[[1]]
# bind rows
all_state_ppp <- all_state_ppp %>%
bind_rows(info_each_state)
}
# finding the url
all_states_url <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
all_state_urls <- all_states_url %>%
read_html() %>%
html_table()
# table from the url list
state_urls <- all_state_urls[[1]]
# empty dataframe for state info
all_state_ppp <- tibble()
# for loop
for (row_number in 1:nrow(all_state_urls)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- state_urls %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$url
# dataframe with each individual state
info_each_state <- url %>%
read_html() %>%
html_table()
info_each_state <- info_each_state[[1]]
# bind rows
all_state_ppp <- all_state_ppp %>%
bind_rows(info_each_state)
}
# finding the url
all_states_url <- "https://dwillis.github.io/jour472files/ppp_scraping_example/index.html"
# table with all of the urls
all_state_urls <- all_states_url %>%
read_html() %>%
html_table()
# table from the url list
state_urls <- all_state_urls[[1]]
# empty dataframe for state info
all_state_ppp <- tibble()
# for loop
for (row_number in 1:nrow(state_urls)) {
# Keep only the row for a given row number, get rid of every other row
each_row_df <- state_urls %>%
slice(row_number)
# Define url of page to get
url <- each_row_df$url
# dataframe with each individual state
info_each_state <- url %>%
read_html() %>%
html_table()
info_each_state <- info_each_state[[1]]
# bind rows
all_state_ppp <- all_state_ppp %>%
bind_rows(info_each_state)
}
all_state_ppp <- all_state_ppp %>%
mutate(loans_per_capita = total_ppp_loans/population*100000) %>%
arrange(desc(loans_per_capita))
all_state_ppp
